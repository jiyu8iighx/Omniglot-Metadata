# Omniglot 数据清洗
此项目的目标是将记录全世界各书写系统信息的[Omniglot百科站点](https://www.omniglot.com/)中的页面和文件中的数据清洗成一个结构化的数据集，尤其是使其中提及的语言和书写系统与[ISO 639-3标准](https://iso639-3.sil.org/code_tables/download_tables)与[ISO 15924标准](https://www.unicode.org/iso15924/iso15924-codes.html)代码配对。
我已爬取一些HTML和XLS/XLSX文件，并为便于你处理已自行解析并创建一些衍生文件，它们都置于当前目录下。

## 行动准则
你将为此项目的实现进行长远的计划，为此你的一般行动准则是：
* 行动之前必须做（哪怕简短的）计划，一是为了更好地行动，二是便于我及早地阻止你浪费时间和资源。
* 由于与你对话的上下文资源有限，你须先考虑什么样的准备工作可以更高效地完成任务。比如：
  * 相比于读入整个文件，你更应考虑如何只传入有用的部分。读取HTML源文件并不经济，甚至它的长度可能超过最大允许的上下文长度。有时目标文件的行结构（比如紧密布局的HTML或JSON）不利于`grep`类命令，你可以考虑先实现（或调用已成熟的）parse或format工具来预处理要读入的文件，生成更利于多次重复查询的文件内容。
* 倾向于每一步做同质化的事情，比如80%的文件可以被当前步骤的策略正常处理，不要在处理时混入对剩下20%文件的例外的全部读入，而是将这些例外记录下来留给下一步来观察、计划、处理。
* 实行计划之前，先请求批准，也就是说等待我的回应再真正付诸行动。尤其是，你希望做的一些工作我可能已经做过，请求批准可以得到我的反馈而避免重复劳作。
* 计算密集型任务应由我执行：字符串相似度匹配、大规模数据处理等高性能压力的计算会导致会话超时中断，应当准备好脚本供我手动执行，而不是在会话中直接运行。
* 不要做没有可靠的实现方法的数据结构设计。
* 处理源数据时应时刻牢记这些数据是待清洗的脏数据，其中很可能有错误，所以不应完全信任包括索引页面在内的信息。得到正确信息的方式包括通过多个来源交叉校验、读取源页面内容来获取额外信息，最终手段是筛选出少数无法可靠地自动处理的交给我来处理。由于我计划将所有表格文件结合你制成的元数据送往其他LLM处理，这也将是一个修正错误的机会，因为该LLM会读取表格文件内容，而表中可能有更多信息（当然如果需要修正信息的条目很少，你也可以自己直接读取表格以寻求更多信息）。
* 我的系统中安装有几乎所有主流程序语言的工具链，尽管其中一些环境你没有权限接触。需要时你可选择合适的语言撰写功能清晰、易维护的脚本。我的偏好是`Python,Rust,Wolfram`，不过由于LLM在`Wolfram`语言上表现尚不好，不要尝试用它执行复杂任务而是交给我来实现。
* 为了便于回退和检查，应该避免覆写原有文件，尤其是原始数据应当是只读的（列于`.gitignore`中，意味着我预期它们不会变更）。如果数据有错误，应当提供可选的、补丁式的修复。最终调用数据时可能是一堆补丁或映射的嵌套。
* 考虑用户进行代码、数据审计的经济性，尽可能注重程序结构和细节实现的简洁，如：
  * 在撰写程序时，不要用大量的`print`来输出不必用到程序输出结果的信息，如果是注释就用程序语言内建的注释支持，该在会话中用文本回答就回答，不是程序的东西不要写到程序源码里去。
  * 数据结构中不加不必要的字段，尤其是内容冗长、处于较深层次的（层次深意味着过滤用的代码也更复杂）。
  * 如果你的过去程序实现被认为是不可靠的，你可以覆盖自己的实现（当然，如果是过去已接受的、稳定的功能就不要动）而不是创建新的文件，杜绝`new_xxx_corrected_final_version.py`这样的命名。如果你进行的修改要实现的是与之前相同的目标，输出的文件不要改新名字。否则你在修正程序错误时，会创建大量无用的文件。

## 目录当前状态
### ISO标准数据（ISO/目录）
* `iso-639-3.tsv`是ISO 639-3完整标准代码表（原始数据，7923条）。
* `iso-639-3-cleaned-simple.tsv`是清洗后的简化版代码表（7919条，仅Id/Scope/Ref_Name三列，已移除4个Special代码）。
* `iso-639-3-macrolanguage-hierarchy.json`是宏语言隶属关系结构化数据（63个宏语言，443个成员语言映射）。
* `iso15924-codes.tsv`是ISO 15924书写系统标准代码表。
* `languages.htm`是原站点路径为`/writing/languages.htm`的页面内容，其中列有Omniglot网站中被认为是语言的词条。其中所提及的页面已下载到`language/`目录下。其中的词条链接我已提取为「链接目标」和「链接标签」两列，置于CSV文件`language.csv`中。
* `index.htm`是原站点路径为`/writing/index.htm`的页面内容，其中列有Omniglot网站中被认为是书写系统的词条。其中所提及的页面已下载到`writing/`目录下。其中的词条链接我已提取为「链接目标」和「链接标签」两列，置于CSV文件`writing.csv`中。
* 由于一个页面中可能同时包含语言和书写系统信息，所以上述两种链接目标可能是相同页面，也就是说`writing.csv`中的「链接目标」列可能与`language.csv`中的「链接目标」列有重复。
* `langalph.htm`是原站点路径为`/writing/langalph.htm`的页面内容，其中给出了书写系统与它所用于拼写的语言。
  * 一方面它列出了那些只用于书写一种语言的书写系统。其中的词条链接我已提取为「链接目标」和「链接标签」两列，置于CSV文件`langalphSingle.csv`中。
  * 另一方面它给出了书写多种语言的书写系统与那些语言间的对应关系。我已解析这部分内容置于`langalphMap.json`中，其结构你须自行查看。
* `charts.html`是原站点路径为`/charts/`的页面内容，其中列有XLS/XLSX表格文件，这些表格文件中可能有很多类型的信息（顺便一提，其中对我来说最重要的是IPA与字母的对应关系）。然而这个列表本身有许多错误且不够全，我已下载**更多**的表格文件置于`charts/`目录下，这些文件的原本路径形如`/charts/someFileName.xls`。由于`xls/xlsx`不是明文格式，我使用`ssconvert`为你生成了CSV格式的数据，以`ssconvert`默认的命名规则置于`csv/`目录下。
* `erratum.md`中有两个版本的勘误表。v1是过去已提交给网站维护者的，大多已订正可以忽略；v2是原本计划在之后提供给网站维护者的勘误表，其中列有已知错误，其中一些错误已在我提取的文件中修正，但HTML等源文件中未修正。这些内容仅供你参考以了解存在什么类型的错误。

## 零散问题
* 同一链接目标可能对应不同类型实体，如书写系统、语言、正字法/转写法。即便是同类型实体，在不同页面可能也有不同标签，如何处理它们？
* **ISO映射的层次问题**: 简单的字符串相似度匹配会将通用系统页面(如"Latin")误匹配到具体变体(如"Latin Fraktur variant")，需要建立主系统-变体的层次关系来避免此类错误。
